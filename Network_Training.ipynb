{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "from src.Dataset import HiC_Dataset\n",
    "from src.layers.WEGATConv import WEGAT_TOPK_Conv\n",
    "from src.layers.utils import PositionalEncoding\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter, Linear, Sequential, Dropout\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import torch_geometric as tgm\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import TopKPooling as TKP\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "DATASET = \"Data/test_dset_18features_custom_norm.pt\"\n",
    "NUMEPOCHS = 10\n",
    "BATCHSIZE = 500\n",
    "LEARNING_RATE = 0.00005\n",
    "WEIGHT_DECAY = 5e-4\n",
    "MANUAL_SEED = 40\n",
    "TRAIN_FRACTION = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UTILITY FUNCTIONS\n",
    "'''\n",
    "def get_middle_features(x,\n",
    "                        numnodes = 51\n",
    "                       ):\n",
    "    mid = int((numnodes-1)/2)\n",
    "    idxs = torch.arange(mid, x.shape[0], numnodes)\n",
    "    return x[idxs,:]\n",
    "\n",
    "\n",
    "'''\n",
    "WEIGHTED EDGE GRAPH ATTENTION MODULE\n",
    "'''\n",
    "class WEGATModule(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_channels=20,\n",
    "                 numchip = 18,\n",
    "                 numedge = 3,\n",
    "                 heads = 4,\n",
    "                 num_graph_convs = 6,\n",
    "                 embedding_layers = 5,\n",
    "                 num_fc = 8,\n",
    "                 fc_channels = [15,15,15,10,10,10,5,2],\n",
    "                 num_prom_fc = 10,\n",
    "                 prom_fc_channels = [15,15,15,15,10,10,10,10,5,2],\n",
    "                 positional_encoding = True,\n",
    "                 pos_embedding_dropout = 0.1,\n",
    "                 fc_dropout = 0.5,\n",
    "                 conv_dropout = 0.1\n",
    "                ):\n",
    "        if isinstance(fc_channels,int):\n",
    "            fc_channels = [fc_channels]*num_fc\n",
    "        elif len(fc_channels) != num_fc:\n",
    "            print(\"number of fully connected channels must match the number of fully connected layers\")\n",
    "            raise\n",
    "\n",
    "        if num_graph_convs < 1:\n",
    "            print(\"need at least one graph convolution\")\n",
    "            raise\n",
    "        num_graph_convs = int(num_graph_convs)\n",
    "\n",
    "        if isinstance(prom_fc_channels,int):\n",
    "            prom_fc_channels = [prom_fc_channels]*num_prom_fc\n",
    "        elif len(prom_fc_channels) != num_prom_fc:\n",
    "            raise\n",
    "\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        #dropout layer\n",
    "        self.dropout = Dropout(p=fc_dropout)\n",
    "\n",
    "        #number of input chip features\n",
    "        self.numchip = numchip\n",
    "\n",
    "        #Whether to apply positional encoding to nodes\n",
    "        self.positional_encoding = positional_encoding\n",
    "        if positional_encoding:\n",
    "            self.posencoder = PositionalEncoding(hidden_channels,\n",
    "                                                 dropout=pos_embedding_dropout,\n",
    "                                                 identical_sizes = True\n",
    "                                                )\n",
    "\n",
    "        #initial embeddding layer\n",
    "        embedding = []\n",
    "        embedding.append(Linear(numchip,\n",
    "                                hidden_channels)\n",
    "                        )\n",
    "        embedding.append(torch.nn.Dropout(p=fc_dropout))\n",
    "        embedding.append(torch.nn.ReLU())\n",
    "        for idx in torch.arange(embedding_layers - 1):\n",
    "            embedding.append(Linear(hidden_channels,\n",
    "                                    hidden_channels)\n",
    "                            )\n",
    "            embedding.append(torch.nn.Dropout(p=fc_dropout))\n",
    "            embedding.append(torch.nn.ReLU())\n",
    "        self.embedding = Sequential(*embedding)\n",
    "\n",
    "        #graph convolution layers\n",
    "        gconv = []\n",
    "        for idx in np.arange(num_graph_convs):\n",
    "            gconv.append(WEGAT_TOPK_Conv(node_inchannels = hidden_channels,\n",
    "                                     node_outchannels = hidden_channels,\n",
    "                                     edge_inchannels = numedge,\n",
    "                                     edge_outchannels = numedge,\n",
    "                                     heads = heads,\n",
    "                                     dropout = conv_dropout\n",
    "                                    )\n",
    "                        )\n",
    "\n",
    "        self.gconv = Sequential(*gconv)\n",
    "\n",
    "        #fully connected channels\n",
    "        fc_channels = [hidden_channels]+fc_channels\n",
    "        lin = []\n",
    "        for idx in torch.arange(num_fc):\n",
    "            lin.append(Linear(fc_channels[idx],fc_channels[idx+1]))\n",
    "            lin.append(torch.nn.Dropout(p=fc_dropout))\n",
    "            lin.append(torch.nn.ReLU())\n",
    "        self.lin = Sequential(*lin)\n",
    "        self.num_fc = num_fc\n",
    "\n",
    "        #fully connected promoter channels\n",
    "        prom_fc_channels = [numchip]+prom_fc_channels\n",
    "        linprom = []\n",
    "        for idx in torch.arange(num_prom_fc):\n",
    "            linprom.append(Linear(prom_fc_channels[idx],prom_fc_channels[idx+1]))\n",
    "            linprom.append(torch.nn.Dropout(p=fc_dropout))\n",
    "            linprom.append(torch.nn.ReLU())\n",
    "        self.linprom = Sequential(*linprom)\n",
    "        self.num_prom_fc = num_prom_fc\n",
    "\n",
    "        #final readout function\n",
    "        self.readout = Linear(prom_fc_channels[-1]+fc_channels[-1], 1)\n",
    "\n",
    "    def forward(self,\n",
    "                batch):\n",
    "        batch.prom_x = batch.prom_x.view(-1,self.numchip).float()\n",
    "        batch.edge_attr[torch.isnan(batch.edge_attr)] = 0\n",
    "        batch.x[torch.isnan(batch.x)] = 0\n",
    "        batch.prom_x[torch.isnan(batch.prom_x)] = 0\n",
    "        \n",
    "        #initial dropout and embedding\n",
    "        batch.x = self.dropout(batch.x)\n",
    "        batch.x = self.embedding(batch.x.float())\n",
    "\n",
    "        #positional encoding\n",
    "        if self.positional_encoding:\n",
    "            batch.x = self.posencoder(batch.x,\n",
    "                                      batch.batch)\n",
    "\n",
    "        #graph convolutions\n",
    "        batch = self.gconv(batch)\n",
    "\n",
    "        #extracting node of interest from graph\n",
    "        x = get_middle_features(batch.x)\n",
    "\n",
    "        # 3. Apply fully connected linear layers to graph\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # 3. Apply fully connected linear layers to promoter\n",
    "        prom_x = self.linprom(batch.prom_x)\n",
    "\n",
    "        r_x = torch.cat([x,prom_x],\n",
    "                        dim = 1)\n",
    "        \n",
    "        # 4. Apply readout layers\n",
    "        x = self.readout(r_x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LIGHTNING NET\n",
    "'''\n",
    "class LitWEGATNet(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 module,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 learning_rate,\n",
    "                 numsteps\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.numsteps = numsteps\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    def validation_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        pred = self.module(batch).squeeze()\n",
    "        idxs = abs(batch.y.float())>0.01\n",
    "        loss = F.l1_loss(pred[idxs], batch.y.float()[idxs])\n",
    "        return loss, pred\n",
    "\n",
    "    def customlog(self, name, loss, pred):\n",
    "        self.log(f'{name}_loss', loss)\n",
    "        self.log(f'{name}_maxabs_prediction',\n",
    "                 torch.max(abs(pred)).item())\n",
    "        self.log(f'{name}_mean_prediction',\n",
    "                 torch.mean(pred).item())\n",
    "        self.log(f'{name}_std_prediction',\n",
    "                 torch.std(pred).item())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred = self.shared_step(batch)\n",
    "        self.customlog('train',loss, pred)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred = self.shared_step(batch)\n",
    "        self.customlog('val',loss, pred)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, pred = self.shared_step(batch)\n",
    "        self.customlog('test',loss, pred)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                     lr=self.learning_rate)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': OneCycleLR(optimizer,\n",
    "                                        max_lr = 10*self.learning_rate,\n",
    "                                        total_steps = self.numsteps\n",
    "                                       )\n",
    "            }\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in memory datasets\n",
      "Loaded in memory datasets\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CONSTRUCTING THE DATALOADERS\n",
    "'''\n",
    "print(\"Loading in memory datasets\")\n",
    "dset = torch.load(DATASET,map_location=torch.device('cpu'))\n",
    "\n",
    "numdatapoints = len(dset)\n",
    "trainsize = int(numdatapoints*TRAIN_FRACTION)\n",
    "train_dset, val_dset = random_split(dset,\n",
    "                                    [trainsize, numdatapoints-trainsize],\n",
    "                                    generator=torch.Generator().manual_seed(MANUAL_SEED)\n",
    "                                   )\n",
    "\n",
    "print(\"Loaded in memory datasets\")\n",
    "train_loader = DataLoader(train_dset, \n",
    "                              batch_size=BATCHSIZE,\n",
    "                              num_workers=20\n",
    "                             )\n",
    "val_loader = DataLoader(val_dset, \n",
    "                             batch_size=BATCHSIZE,\n",
    "                            num_workers=20\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvals = []\\ni = 0\\nfor dat in train_loader:\\n    vals.append(dat.y.detach().numpy())\\n    i+=1\\n    if i > 500:\\n        break\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vals = []\n",
    "i = 0\n",
    "for dat in train_loader:\n",
    "    vals.append(dat.y.detach().numpy())\n",
    "    i+=1\n",
    "    if i > 500:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvals = np.concatenate(vals)\\nvals = vals[~((vals<0.01)&(vals>-0.01))]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vals = np.concatenate(vals)\n",
    "vals = vals[~((vals<0.01)&(vals>-0.01))]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith plt.xkcd():\\n    fig = plt.figure()\\n    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\\n    ax.spines['right'].set_color('none')\\n    ax.spines['top'].set_color('none')\\n    #ax.set_xticks([])\\n    ax.set_yticks([])\\n    h=ax.hist(vals,bins = 100)\\n    ax.set_xlim([-0.5,0.5])\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with plt.xkcd():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    #ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    h=ax.hist(vals,bins = 100)\n",
    "    ax.set_xlim([-0.5,0.5])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nidx = np.argmax(h[0].astype('int'))\\nh[1][idx:idx+2]\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "idx = np.argmax(h[0].astype('int'))\n",
    "h[1][idx:idx+2]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################Layer##########################\n",
      "%%%%%%%%%%%%%%%%%%%%%%inputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t-0.9999129772186279\n",
      "\tmax:\t1.2642322778701782\n",
      "\tmean:\t0.42540255188941956\n",
      "Edges:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t2182.119685648557\n",
      "\tmean:\t14.522982525895088\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Pre-processed node features\n",
      "\tmin:\t-0.9999129772186279\n",
      "\tmax:\t1.2642322778701782\n",
      "\tmean:\t0.42540255188941956\n",
      "pre-processed edge features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t2182.11962890625\n",
      "\tmean:\t14.52298355102539\n",
      "embedded node features\n",
      "\tmin:\t-1.3763574361801147\n",
      "\tmax:\t1.468764305114746\n",
      "\tmean:\t-0.008575689978897572\n",
      "embedded edge features\n",
      "\tmin:\t-1503.5260009765625\n",
      "\tmax:\t1290.4683837890625\n",
      "\tmean:\t-7.8589324951171875\n",
      "edge attention coefficients\n",
      "\tmin:\t-662.9281616210938\n",
      "\tmax:\t2907.098388671875\n",
      "\tmean:\t4.160726547241211\n",
      "node attention coefficients\n",
      "\tmin:\t-1.177519679069519\n",
      "\tmax:\t1.3119288682937622\n",
      "\tmean:\t-0.02697741985321045\n",
      "edge bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "node bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "propagated out node features (embedded features + bias)\n",
      "\tmin:\t-0.34558409452438354\n",
      "\tmax:\t1.4668900966644287\n",
      "\tmean:\t0.4433160126209259\n",
      "propagated out edge features (embedded features + bias)\n",
      "\tmin:\t-214.97637939453125\n",
      "\tmax:\t1290.4683837890625\n",
      "\tmean:\t5.685280799865723\n",
      "%%%%%%%%%%%%%%%%%convolved outputs%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%pre dropout/relu%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t-0.34558409452438354\n",
      "\tmax:\t1.4668900966644287\n",
      "\tmean:\t0.4433160126209259\n",
      "Edges:\n",
      "\tmin:\t-214.97637939453125\n",
      "\tmax:\t1290.4683837890625\n",
      "\tmean:\t5.685280799865723\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%%%%%%outputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1.4668900966644287\n",
      "\tmean:\t0.4525608718395233\n",
      "Edges:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1290.4683837890625\n",
      "\tmean:\t6.017221927642822\n",
      "#######################Layer##########################\n",
      "%%%%%%%%%%%%%%%%%%%%%%inputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1.4668900966644287\n",
      "\tmean:\t0.4525608718395233\n",
      "Edges:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1290.4683837890625\n",
      "\tmean:\t6.017221927642822\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Pre-processed node features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1.4668900966644287\n",
      "\tmean:\t0.4525608718395233\n",
      "pre-processed edge features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1290.4683837890625\n",
      "\tmean:\t6.017221927642822\n",
      "embedded node features\n",
      "\tmin:\t-1.1959023475646973\n",
      "\tmax:\t1.3823528289794922\n",
      "\tmean:\t-0.06618612259626389\n",
      "embedded edge features\n",
      "\tmin:\t-508.7686767578125\n",
      "\tmax:\t1040.7999267578125\n",
      "\tmean:\t0.9479011297225952\n",
      "edge attention coefficients\n",
      "\tmin:\t-358.094970703125\n",
      "\tmax:\t219.63870239257812\n",
      "\tmean:\t-0.9516106843948364\n",
      "node attention coefficients\n",
      "\tmin:\t-0.558643639087677\n",
      "\tmax:\t0.5780321359634399\n",
      "\tmean:\t-0.007554417010396719\n",
      "edge bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "node bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "propagated out node features (embedded features + bias)\n",
      "\tmin:\t-0.34624987840652466\n",
      "\tmax:\t1.3817343711853027\n",
      "\tmean:\t0.29759612679481506\n",
      "propagated out edge features (embedded features + bias)\n",
      "\tmin:\t-10.382850646972656\n",
      "\tmax:\t1040.7999267578125\n",
      "\tmean:\t6.667301177978516\n",
      "%%%%%%%%%%%%%%%%%convolved outputs%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%pre dropout/relu%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t-0.34624987840652466\n",
      "\tmax:\t1.3817343711853027\n",
      "\tmean:\t0.29759612679481506\n",
      "Edges:\n",
      "\tmin:\t-10.382850646972656\n",
      "\tmax:\t1040.7999267578125\n",
      "\tmean:\t6.667301177978516\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%%%%%%outputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1.3817343711853027\n",
      "\tmean:\t0.3101412057876587\n",
      "Edges:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1040.7999267578125\n",
      "\tmean:\t6.684764862060547\n",
      "#######################Layer##########################\n",
      "%%%%%%%%%%%%%%%%%%%%%%inputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1.3817343711853027\n",
      "\tmean:\t0.3101412057876587\n",
      "Edges:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1040.7999267578125\n",
      "\tmean:\t6.684764862060547\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Pre-processed node features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1.3817343711853027\n",
      "\tmean:\t0.3101412057876587\n",
      "pre-processed edge features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t1040.7999267578125\n",
      "\tmean:\t6.684764862060547\n",
      "embedded node features\n",
      "\tmin:\t-0.950002908706665\n",
      "\tmax:\t0.8091758489608765\n",
      "\tmean:\t0.00883797463029623\n",
      "embedded edge features\n",
      "\tmin:\t-552.9534912109375\n",
      "\tmax:\t441.3499450683594\n",
      "\tmean:\t-0.9470361471176147\n",
      "edge attention coefficients\n",
      "\tmin:\t-222.56190490722656\n",
      "\tmax:\t479.24810791015625\n",
      "\tmean:\t1.1919294595718384\n",
      "node attention coefficients\n",
      "\tmin:\t-0.2274158000946045\n",
      "\tmax:\t0.7024452686309814\n",
      "\tmean:\t0.2274509072303772\n",
      "edge bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "node bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "propagated out node features (embedded features + bias)\n",
      "\tmin:\t-0.03638600930571556\n",
      "\tmax:\t0.8074948191642761\n",
      "\tmean:\t0.2827148139476776\n",
      "propagated out edge features (embedded features + bias)\n",
      "\tmin:\t0.004270084667950869\n",
      "\tmax:\t441.3499450683594\n",
      "\tmean:\t2.4347352981567383\n",
      "%%%%%%%%%%%%%%%%%convolved outputs%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%pre dropout/relu%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t-0.03638600930571556\n",
      "\tmax:\t0.8074948191642761\n",
      "\tmean:\t0.2827148139476776\n",
      "Edges:\n",
      "\tmin:\t0.004270084667950869\n",
      "\tmax:\t441.3499450683594\n",
      "\tmean:\t2.4347352981567383\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%%%%%%outputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.8074948191642761\n",
      "\tmean:\t0.2827509045600891\n",
      "Edges:\n",
      "\tmin:\t0.004270084667950869\n",
      "\tmax:\t441.3499450683594\n",
      "\tmean:\t2.4347352981567383\n",
      "#######################Layer##########################\n",
      "%%%%%%%%%%%%%%%%%%%%%%inputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.8074948191642761\n",
      "\tmean:\t0.2827509045600891\n",
      "Edges:\n",
      "\tmin:\t0.004270084667950869\n",
      "\tmax:\t441.3499450683594\n",
      "\tmean:\t2.4347352981567383\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Pre-processed node features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.8074948191642761\n",
      "\tmean:\t0.2827509045600891\n",
      "pre-processed edge features\n",
      "\tmin:\t0.004270084667950869\n",
      "\tmax:\t441.3499450683594\n",
      "\tmean:\t2.4347352981567383\n",
      "embedded node features\n",
      "\tmin:\t-0.7945341467857361\n",
      "\tmax:\t0.5827491283416748\n",
      "\tmean:\t-0.024339430034160614\n",
      "embedded edge features\n",
      "\tmin:\t-510.3872985839844\n",
      "\tmax:\t453.02740478515625\n",
      "\tmean:\t0.5200023651123047\n",
      "edge attention coefficients\n",
      "\tmin:\t-260.1125793457031\n",
      "\tmax:\t343.1427001953125\n",
      "\tmean:\t0.6275489330291748\n",
      "node attention coefficients\n",
      "\tmin:\t-0.29459744691848755\n",
      "\tmax:\t0.3658364415168762\n",
      "\tmean:\t0.028368499130010605\n",
      "edge bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "node bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "propagated out node features (embedded features + bias)\n",
      "\tmin:\t-0.11031750589609146\n",
      "\tmax:\t0.5827489495277405\n",
      "\tmean:\t0.1794341802597046\n",
      "propagated out edge features (embedded features + bias)\n",
      "\tmin:\t0.012583933770656586\n",
      "\tmax:\t453.02740478515625\n",
      "\tmean:\t2.784452438354492\n",
      "%%%%%%%%%%%%%%%%%convolved outputs%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%pre dropout/relu%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t-0.11031750589609146\n",
      "\tmax:\t0.5827489495277405\n",
      "\tmean:\t0.1794341802597046\n",
      "Edges:\n",
      "\tmin:\t0.012583933770656586\n",
      "\tmax:\t453.02740478515625\n",
      "\tmean:\t2.784452438354492\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%%%%%%outputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.5827489495277405\n",
      "\tmean:\t0.18178144097328186\n",
      "Edges:\n",
      "\tmin:\t0.012583933770656586\n",
      "\tmax:\t453.02740478515625\n",
      "\tmean:\t2.784452438354492\n",
      "#######################Layer##########################\n",
      "%%%%%%%%%%%%%%%%%%%%%%inputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.5827489495277405\n",
      "\tmean:\t0.18178144097328186\n",
      "Edges:\n",
      "\tmin:\t0.012583933770656586\n",
      "\tmax:\t453.02740478515625\n",
      "\tmean:\t2.784452438354492\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Pre-processed node features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.5827489495277405\n",
      "\tmean:\t0.18178144097328186\n",
      "pre-processed edge features\n",
      "\tmin:\t0.012583933770656586\n",
      "\tmax:\t453.02740478515625\n",
      "\tmean:\t2.784452438354492\n",
      "embedded node features\n",
      "\tmin:\t-0.540703535079956\n",
      "\tmax:\t0.4643010199069977\n",
      "\tmean:\t0.0006396188982762396\n",
      "embedded edge features\n",
      "\tmin:\t-329.5263671875\n",
      "\tmax:\t331.1233825683594\n",
      "\tmean:\t-0.6404514908790588\n",
      "edge attention coefficients\n",
      "\tmin:\t-265.55035400390625\n",
      "\tmax:\t278.33331298828125\n",
      "\tmean:\t-0.058520469814538956\n",
      "node attention coefficients\n",
      "\tmin:\t-0.5087888836860657\n",
      "\tmax:\t0.38323208689689636\n",
      "\tmean:\t0.006344865541905165\n",
      "edge bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "node bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "propagated out node features (embedded features + bias)\n",
      "\tmin:\t-0.05241187661886215\n",
      "\tmax:\t0.4643009901046753\n",
      "\tmean:\t0.16733473539352417\n",
      "propagated out edge features (embedded features + bias)\n",
      "\tmin:\t0.007110858801752329\n",
      "\tmax:\t331.1233825683594\n",
      "\tmean:\t1.693724274635315\n",
      "%%%%%%%%%%%%%%%%%convolved outputs%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%pre dropout/relu%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t-0.05241187661886215\n",
      "\tmax:\t0.4643009901046753\n",
      "\tmean:\t0.16733473539352417\n",
      "Edges:\n",
      "\tmin:\t0.007110858801752329\n",
      "\tmax:\t331.1233825683594\n",
      "\tmean:\t1.693724274635315\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%%%%%%outputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.4643009901046753\n",
      "\tmean:\t0.16812404990196228\n",
      "Edges:\n",
      "\tmin:\t0.007110858801752329\n",
      "\tmax:\t331.1233825683594\n",
      "\tmean:\t1.693724274635315\n",
      "#######################Layer##########################\n",
      "%%%%%%%%%%%%%%%%%%%%%%inputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.4643009901046753\n",
      "\tmean:\t0.16812404990196228\n",
      "Edges:\n",
      "\tmin:\t0.007110858801752329\n",
      "\tmax:\t331.1233825683594\n",
      "\tmean:\t1.693724274635315\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Pre-processed node features\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.4643009901046753\n",
      "\tmean:\t0.16812404990196228\n",
      "pre-processed edge features\n",
      "\tmin:\t0.007110858801752329\n",
      "\tmax:\t331.1233825683594\n",
      "\tmean:\t1.693724274635315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded node features\n",
      "\tmin:\t-0.38680407404899597\n",
      "\tmax:\t0.44472649693489075\n",
      "\tmean:\t0.009539792314171791\n",
      "embedded edge features\n",
      "\tmin:\t-206.00567626953125\n",
      "\tmax:\t160.3920135498047\n",
      "\tmean:\t0.2013828009366989\n",
      "edge attention coefficients\n",
      "\tmin:\t-196.41159057617188\n",
      "\tmax:\t40.31742858886719\n",
      "\tmean:\t-0.8054631948471069\n",
      "node attention coefficients\n",
      "\tmin:\t-0.2830515503883362\n",
      "\tmax:\t0.3601614534854889\n",
      "\tmean:\t0.06101009622216225\n",
      "edge bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "node bias\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.0\n",
      "\tmean:\t0.0\n",
      "propagated out node features (embedded features + bias)\n",
      "\tmin:\t-0.03537609428167343\n",
      "\tmax:\t0.4446447491645813\n",
      "\tmean:\t0.1373981237411499\n",
      "propagated out edge features (embedded features + bias)\n",
      "\tmin:\t0.004849775228649378\n",
      "\tmax:\t160.3920135498047\n",
      "\tmean:\t1.0535961389541626\n",
      "%%%%%%%%%%%%%%%%%convolved outputs%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%pre dropout/relu%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t-0.03537609428167343\n",
      "\tmax:\t0.4446447491645813\n",
      "\tmean:\t0.1373981237411499\n",
      "Edges:\n",
      "\tmin:\t0.004849775228649378\n",
      "\tmax:\t160.3920135498047\n",
      "\tmean:\t1.0535961389541626\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "%%%%%%%%%%%%%%%%%%%%%%outputs%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Nodes:\n",
      "\tmin:\t0.0\n",
      "\tmax:\t0.4446447491645813\n",
      "\tmean:\t0.13855630159378052\n",
      "Edges:\n",
      "\tmin:\t0.004849775228649378\n",
      "\tmax:\t160.3920135498047\n",
      "\tmean:\t1.0535961389541626\n"
     ]
    }
   ],
   "source": [
    "NUMCHIP = dset[0].x.shape[1]\n",
    "NUMEDGE = dset[0].edge_attr.shape[1]\n",
    "module = WEGATModule(hidden_channels = 20,\n",
    "                         numchip = NUMCHIP,\n",
    "                         numedge = NUMEDGE,\n",
    "                         embedding_layers = 4,\n",
    "                         positional_encoding = True,\n",
    "                         pos_embedding_dropout = 0,\n",
    "                         fc_dropout = 0,\n",
    "                         conv_dropout = 0\n",
    "                        )\n",
    "for dat in val_loader:\n",
    "    module(dat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name   | Type        | Params\n",
      "---------------------------------------\n",
      "0 | module | WEGATModule | 15.4 K\n",
      "---------------------------------------\n",
      "15.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.4 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X pre processing\n",
      "tensor(0.3971, dtype=torch.float64) tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n",
      "X post embedding\n",
      "tensor(0.0427) tensor(0.2669) tensor(0.)\n",
      "X post positional encoding\n",
      "tensor(0.4254) tensor(1.2642) tensor(-0.9999)\n",
      "X post graph convolutions\n",
      "tensor(0.) tensor(0.) tensor(0.)\n",
      "X post processing\n",
      "tensor(0.) tensor(0.) tensor(0.)\n",
      "X pre processing\n",
      "tensor(0.3866, dtype=torch.float64) tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n",
      "X post embedding\n",
      "tensor(0.0428) tensor(0.2668) tensor(0.)\n",
      "X post positional encoding\n",
      "tensor(0.4254) tensor(1.2651) tensor(-0.9999)\n",
      "X post graph convolutions\n",
      "tensor(0.) tensor(0.) tensor(0.)\n",
      "X post processing\n",
      "tensor(0.) tensor(0.) tensor(0.)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003ff25155444cc7bf4e6c1e78c38e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X pre processing\n",
      "tensor(0.3989, dtype=torch.float64) tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n",
      "X post embedding\n",
      "tensor(0.2408, grad_fn=<MeanBackward0>) tensor(430.0927, grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post positional encoding\n",
      "tensor(0.6270, grad_fn=<MeanBackward0>) tensor(3252.6562, grad_fn=<MaxBackward1>) tensor(-9.9999, grad_fn=<MinBackward1>)\n",
      "X post graph convolutions\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post processing\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X pre processing\n",
      "tensor(0.4027, dtype=torch.float64) tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n",
      "X post embedding\n",
      "tensor(0.2327, grad_fn=<MeanBackward0>) tensor(391.0336, grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post positional encoding\n",
      "tensor(0.6108, grad_fn=<MeanBackward0>) tensor(1876.0699, grad_fn=<MaxBackward1>) tensor(-9.9999, grad_fn=<MinBackward1>)\n",
      "X post graph convolutions\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post processing\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X pre processing\n",
      "tensor(0.4061, dtype=torch.float64) tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n",
      "X post embedding\n",
      "tensor(0.2377, grad_fn=<MeanBackward0>) tensor(617.4684, grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post positional encoding\n",
      "tensor(0.6193, grad_fn=<MeanBackward0>) tensor(2050.0190, grad_fn=<MaxBackward1>) tensor(-9.9999, grad_fn=<MinBackward1>)\n",
      "X post graph convolutions\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post processing\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X pre processing\n",
      "tensor(0.4070, dtype=torch.float64) tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n",
      "X post embedding\n",
      "tensor(0.2404, grad_fn=<MeanBackward0>) tensor(536.1006, grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post positional encoding\n",
      "tensor(0.6605, grad_fn=<MeanBackward0>) tensor(5279.4756, grad_fn=<MaxBackward1>) tensor(-9.9999, grad_fn=<MinBackward1>)\n",
      "X post graph convolutions\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post processing\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X pre processing\n",
      "tensor(0.4067, dtype=torch.float64) tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n",
      "X post embedding\n",
      "tensor(0.2521, grad_fn=<MeanBackward0>) tensor(486.3310, grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post positional encoding\n",
      "tensor(0.6215, grad_fn=<MeanBackward0>) tensor(2163.1785, grad_fn=<MaxBackward1>) tensor(-9.9999, grad_fn=<MinBackward1>)\n",
      "X post graph convolutions\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "X post processing\n",
      "tensor(0., grad_fn=<MeanBackward0>) tensor(0., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x2b5aba7e23c8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dh486/gnn-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dh486/gnn-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/software/master/python/3.6/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/local/software/master/python/3.6/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/local/software/master/python/3.6/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/software/master/python/3.6/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "NUMCHIP = dset[0].x.shape[1]\n",
    "NUMEDGE = dset[0].edge_attr.shape[1]\n",
    "\n",
    "module = WEGATModule(hidden_channels = 20,\n",
    "                         numchip = NUMCHIP,\n",
    "                         numedge = NUMEDGE,\n",
    "                         embedding_layers = 4,\n",
    "                         positional_encoding = True,\n",
    "                         pos_embedding_dropout = 0.9,\n",
    "                         fc_dropout = 0.9,\n",
    "                         conv_dropout = 0.9\n",
    "                        )\n",
    "Net = LitWEGATNet(module,\n",
    "                  train_loader,\n",
    "                  val_loader,\n",
    "                  LEARNING_RATE,\n",
    "                  50000)\n",
    "    \n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('runs',\n",
    "                                         name = 'printing_test',\n",
    "                                         version = 0\n",
    "                                        )\n",
    "trainer = pl.Trainer(gpus=0,\n",
    "                     max_epochs=100,\n",
    "                     progress_bar_refresh_rate=1,\n",
    "                     #logger=tb_logger,\n",
    "                     auto_lr_find=False)\n",
    "\n",
    "trainer.fit(Net, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 255).\n",
       "Contents of stderr:\n",
       "TensorFlow installation not found - running with reduced feature set.\n",
       "E0519 13:54:06.474670 47233012180864 program.py:311] TensorBoard could not bind to port 8081, it was already in use\n",
       "ERROR: TensorBoard could not bind to port 8081, it was already in use"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir runs/ --port=8081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.laplace import Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(394.5218, dtype=torch.float64, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = WEGATModule()\n",
    "for batch in train_loader:\n",
    "    pred = model(batch).squeeze()\n",
    "    print(-m.log_prob(pred-batch.y).sum())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
