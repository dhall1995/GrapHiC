{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from src.Dataset import HiC_Dataset\n",
    "from src.layers.WEGATConv import WEGATConv\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter, Linear, Sequential\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch_geometric as tgm\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import TopKPooling as TKP\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "DATASET = \"Data/test_dset_18features_custom_norm.pt\"\n",
    "NUMEPOCHS = 10\n",
    "BATCHSIZE = 500\n",
    "LEARNING_RATE = 0.00005\n",
    "WEIGHT_DECAY = 5e-4\n",
    "MANUAL_SEED = 40\n",
    "TRAIN_FRACTION = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "COMBINED 'WEIGHTED EDGE GRAPH ATTENTION' + 'TOP K POOLING LAYERS' \n",
    "'''\n",
    "class WEGAT_TOPK_Conv(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_inchannels,\n",
    "                 node_outchannels,\n",
    "                 edge_inchannels,\n",
    "                 edge_outchannels,\n",
    "                 heads = 4):\n",
    "        super().__init__()\n",
    "        self.conv = WEGATConv(in_channels = node_inchannels, \n",
    "                               node_out_channels = node_outchannels,\n",
    "                               edge_channels = edge_inchannels,\n",
    "                               edge_out_channels = edge_outchannels,\n",
    "                               heads = heads,\n",
    "                               concat = False\n",
    "                              )\n",
    "        self.pool = TKP(in_channels = node_outchannels)\n",
    "        \n",
    "    def forward(self, \n",
    "                dat):\n",
    "        dat['x'], dat['edge_attr'] = self.conv(dat['x'].float(),\n",
    "                                               dat['edge_attr'].float(),\n",
    "                                               dat['edge_index'])\n",
    "        dat['x'] = dat['x'].relu()\n",
    "        dat['edge_attr'] = dat['edge_attr'].relu()\n",
    "        dat['x'], dat['edge_index'], dat['edge_attr'], dat['batch'], perm,score = self.pool(dat['x'],\n",
    "                                                                                           dat['edge_index'],\n",
    "                                                                                           edge_attr = dat['edge_attr'],\n",
    "                                                                                           batch = dat['batch'])\n",
    "        \n",
    "        return dat\n",
    "        \n",
    "\n",
    "\n",
    "'''\n",
    "WEIGHTED EDGE GRAPH ATTENTION MODULE\n",
    "'''\n",
    "class WEGATModule(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_channels,\n",
    "                 numchip = 18,\n",
    "                 numedge = 3,\n",
    "                 heads = 4,\n",
    "                 num_graph_convs = 3,\n",
    "                 num_fc = 5,\n",
    "                 fc_channels = [15,15,10,5,2],\n",
    "                 num_prom_fc = 5,\n",
    "                 prom_fc_channels = [15,15,10,5,2]\n",
    "                ):\n",
    "        if isinstance(fc_channels,int):\n",
    "            fc_channels = [fc_channels]*num_fc\n",
    "        elif len(fc_channels) != num_fc:\n",
    "            print(\"number of fully connected channels must match the number of fully connected layers\")\n",
    "            raise\n",
    "            \n",
    "        if num_graph_convs < 1:\n",
    "            print(\"need at least one graph convolution\")\n",
    "            raise\n",
    "        num_graph_convs = int(num_graph_convs)\n",
    "            \n",
    "        if isinstance(prom_fc_channels,int):\n",
    "            prom_fc_channels = [prom_fc_channels]*num_prom_fc\n",
    "        elif len(prom_fc_channels) != num_prom_fc:\n",
    "            raise\n",
    "        \n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        self.loglikelihood_precision = Parameter(torch.tensor(0.))\n",
    "        gconv = [WEGAT_TOPK_Conv(node_inchannels = numchip, \n",
    "                             node_outchannels = hidden_channels,\n",
    "                             edge_inchannels = numedge,\n",
    "                             edge_outchannels = numedge,\n",
    "                             heads = heads\n",
    "                            )\n",
    "                ]\n",
    "        for idx in np.arange(num_graph_convs-1):\n",
    "            gconv.append(WEGAT_TOPK_Conv(node_inchannels = hidden_channels,\n",
    "                                     node_outchannels = hidden_channels,\n",
    "                                     edge_inchannels = numedge,\n",
    "                                     edge_outchannels = numedge,\n",
    "                                     heads = heads\n",
    "                                    )\n",
    "                        )\n",
    "\n",
    "        self.gconv = Sequential(*gconv)\n",
    "\n",
    "        fc_channels = [hidden_channels]+fc_channels\n",
    "        lin = []\n",
    "        for idx in torch.arange(num_fc):\n",
    "            lin.append(Linear(fc_channels[idx],fc_channels[idx+1]))\n",
    "            lin.append(torch.nn.ReLU())\n",
    "\n",
    "        self.lin = Sequential(*lin)\n",
    "        self.num_fc = num_fc\n",
    "        self.numchip = numchip\n",
    "        \n",
    "        prom_fc_channels = [numchip]+prom_fc_channels\n",
    "        linprom = []\n",
    "        for idx in torch.arange(num_prom_fc):\n",
    "            linprom.append(Linear(prom_fc_channels[idx],prom_fc_channels[idx+1]))\n",
    "            linprom.append(torch.nn.ReLU())\n",
    "\n",
    "        self.linprom = Sequential(*linprom)\n",
    "        self.num_prom_fc = num_prom_fc\n",
    "        \n",
    "        \n",
    "        self.readout = Linear(prom_fc_channels[-1]+fc_channels[-1], 1)\n",
    "        \n",
    "    def forward(self, \n",
    "                x,\n",
    "                edge_index, \n",
    "                edge_attr,\n",
    "                prom_x,\n",
    "                batch):\n",
    "        prom_x = prom_x.view(-1,self.numchip).float()\n",
    "        edge_attr[torch.isnan(edge_attr)] = 0\n",
    "        x[torch.isnan(x)] = 0\n",
    "        prom_x[torch.isnan(prom_x)] = 0\n",
    "        \n",
    "        dat = {}\n",
    "        dat['x'] = x\n",
    "        dat['edge_index'] = edge_index\n",
    "        dat['edge_attr'] = edge_attr\n",
    "        dat['batch'] = batch\n",
    "        \n",
    "        dat = self.gconv(dat)\n",
    "\n",
    "        #global pooling\n",
    "        x = global_max_pool(dat['x'],\n",
    "                            batch=dat['batch'])\n",
    "\n",
    "        # 3. Apply fully connected linear layers to graph\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # 3. Apply fully connected linear layers to promoter\n",
    "        prom_x = self.linprom(prom_x)\n",
    "        \n",
    "        # 4. Apply readout layers \n",
    "        x = self.readout(torch.cat([x,prom_x],\n",
    "                                   dim = 1)\n",
    "                        )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitWEGATNet(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 module,\n",
    "                 learning_rate,\n",
    "                 weight_decay\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.WEGATModule = module\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        pred = self.WEGATModule(batch.x, \n",
    "                             batch.edge_index, \n",
    "                             batch.edge_attr,\n",
    "                             batch.prom_x,\n",
    "                             batch.batch)\n",
    "        \n",
    "        loss = F.l1_loss(pred[:,0],\n",
    "                         batch.y.float())\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss     \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), \n",
    "                                     lr=self.learning_rate,\n",
    "                                     weight_decay = self.weight_decay\n",
    "                                    )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in memory datasets\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TRAINFRACTION' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4071b1197a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnumdatapoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumdatapoints\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTRAINFRACTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m train_dset, val_dset = random_split(dset,\n\u001b[1;32m     10\u001b[0m                                     \u001b[0;34m[\u001b[0m\u001b[0mtrainsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumdatapoints\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrainsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAINFRACTION' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CONSTRUCTING THE DATALOADERS\n",
    "'''\n",
    "print(\"Loading in memory datasets\")\n",
    "dset = torch.load(DATASET,map_location=torch.device('cpu'))\n",
    "\n",
    "numdatapoints = len(dset)\n",
    "trainsize = int(numdatapoints*TRAIN_FRACTION)\n",
    "train_dset, val_dset = random_split(dset,\n",
    "                                    [trainsize, numdatapoints-trainsize],\n",
    "                                    generator=torch.Generator().manual_seed(MANUAL_SEED)\n",
    "                                   )\n",
    "\n",
    "print(\"Loaded in memory datasets\")\n",
    "train_loader = DataLoader(train_dset, \n",
    "                              batch_size=BATCHSIZE,\n",
    "                              num_workers=20\n",
    "                             )\n",
    "val_loader = DataLoader(val_dset, \n",
    "                             batch_size=BATCHSIZE,\n",
    "                            num_workers=20\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMCHIP = dset[0].x.shape[1]\n",
    "NUMEDGE = dset[0].edge_attr.shape[1]\n",
    "    \n",
    "module = WEGATModule(hidden_channels = hparams.hiddenlayers,\n",
    "                      numchip = NUMCHIP,\n",
    "                      numedge = NUMEDGE\n",
    "                     )\n",
    "Net = LitWEGATNet(module, LEARNING_RATE, WEIGHT_D)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(hparams.logdir)\n",
    "trainer = pl.Trainer(gpus=hparams.gpus, \n",
    "                         max_epochs=hparams.epochs, \n",
    "                         progress_bar_refresh_rate=20,\n",
    "                         logger=tb_logger)\n",
    "trainer.fit(Net, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
